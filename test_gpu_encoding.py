"""
–¢–µ—Å—Ç GPU encoding vs CPU
"""
import torch
from sentence_transformers import SentenceTransformer
import time

print("=" * 60)
print("GPU ENCODING TEST")
print("=" * 60)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA
print(f"\nüîπ CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"üîπ GPU: {torch.cuda.get_device_name(0)}")
    print(f"üîπ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
texts = ["–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –æ –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö —É—Å–ª—É–≥–∞—Ö"] * 100

model_name = "intfloat/multilingual-e5-large"

# CPU encoding
print(f"\n‚è±Ô∏è CPU encoding...")
model_cpu = SentenceTransformer(model_name, device='cpu')
start = time.time()
embeddings_cpu = model_cpu.encode(texts, batch_size=32, show_progress_bar=False)
cpu_time = time.time() - start
print(f"   {cpu_time:.2f}s ({len(texts)/cpu_time:.1f} docs/s)")

# GPU encoding
if torch.cuda.is_available():
    print(f"\n‚ö° GPU encoding...")
    model_gpu = SentenceTransformer(model_name, device='cuda')
    
    # Warmup
    _ = model_gpu.encode(texts[:10], batch_size=32, show_progress_bar=False)
    
    start = time.time()
    embeddings_gpu = model_gpu.encode(texts, batch_size=64, show_progress_bar=False)
    gpu_time = time.time() - start
    print(f"   {gpu_time:.2f}s ({len(texts)/gpu_time:.1f} docs/s)")
    
    speedup = cpu_time / gpu_time
    print(f"\nüöÄ Speedup: {speedup:.1f}x faster with GPU")
    
    print(f"\nüìä For 2000 documents:")
    print(f"   CPU: ~{(2000/len(texts)*cpu_time)/60:.1f} minutes")
    print(f"   GPU: ~{(2000/len(texts)*gpu_time)/60:.1f} minutes")

print("\n" + "=" * 60)