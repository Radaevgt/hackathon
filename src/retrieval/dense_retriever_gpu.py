"""
Dense Retrieval —Å GPU acceleration –¥–ª—è encoding
"""

from typing import List, Tuple, Optional
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import torch
from pathlib import Path
from loguru import logger

from src.retrieval.base import BaseRetriever


class DenseRetriever(BaseRetriever):
    """
    Dense retrieval —Å GPU –¥–ª—è encoding, CPU –¥–ª—è FAISS
    
    –í–ê–ñ–ù–û: –ù–∞ Windows FAISS-GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω —á–µ—Ä–µ–∑ pip,
    –ø–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥:
    - GPU –¥–ª—è encoding (10x —É—Å–∫–æ—Ä–µ–Ω–∏–µ)
    - CPU –¥–ª—è FAISS index (–ø–æ–∏—Å–∫ –∏ —Ç–∞–∫ –±—ã—Å—Ç—Ä—ã–π)
    """
    
    def __init__(
        self,
        model_name: str = "intfloat/multilingual-e5-large",
        use_gpu: bool = False,
        batch_size: int = 32,
        normalize_embeddings: bool = True
    ):
        super().__init__(name="DenseRetriever")
        
        self.model_name = model_name
        self.batch_size = batch_size
        self.normalize_embeddings = normalize_embeddings
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA
        self.device = "cpu"
        if use_gpu and torch.cuda.is_available():
            self.device = "cuda"
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            logger.info(f"üöÄ Using GPU: {gpu_name} ({gpu_memory:.1f} GB)")
            logger.info(f"üöÄ GPU encoding will be ~10x faster than CPU")
        elif use_gpu:
            logger.warning("‚ö†Ô∏è GPU requested but CUDA not available, using CPU")
        else:
            logger.info("Using CPU for encoding")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        logger.info(f"Loading model: {model_name}")
        self.model = SentenceTransformer(model_name, device=self.device)
        self.dimension = self.model.get_sentence_embedding_dimension()
        
        self.index = None
        self.num_documents = 0
        
        logger.info(f"Model loaded: embedding_dim={self.dimension}, device={self.device}")
    
    def build_index(
        self,
        documents: List[str],
        index_type: str = "Flat",
        nlist: int = 100
    ):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞ —Å GPU encoding"""
        logger.info(f"Building dense index for {len(documents)} documents...")
        logger.info(f"Index type: {index_type}, Device: {self.device}")
        
        self.num_documents = len(documents)
        
        # üöÄ –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ GPU (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        import time
        start_time = time.time()
        
        embeddings = self._encode_batch(documents, desc="Encoding documents")
        
        encoding_time = time.time() - start_time
        logger.info(f"‚ö° Encoding completed in {encoding_time:.1f}s ({len(documents)/encoding_time:.1f} docs/s)")
        
        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞ (–≤—Å–µ–≥–¥–∞ CPU)
        if index_type == "Flat":
            self.index = self._build_flat_index(embeddings)
        elif index_type == "IVFFlat":
            self.index = self._build_ivf_flat_index(embeddings, nlist)
        elif index_type == "IVFPQ":
            self.index = self._build_ivf_pq_index(embeddings, nlist)
        else:
            raise ValueError(f"Unknown index type: {index_type}")
        
        self.is_built = True
        total_time = time.time() - start_time
        logger.info(f"‚úÖ Dense index built in {total_time:.1f}s: {self.index.ntotal} vectors")
    
    def _build_flat_index(self, embeddings: np.ndarray) -> faiss.Index:
        """Flat –∏–Ω–¥–µ–∫—Å (exact search) - –≤—Å–µ–≥–¥–∞ CPU"""
        index = faiss.IndexFlatIP(self.dimension)
        index.add(embeddings)
        return index
    
    def _build_ivf_flat_index(self, embeddings: np.ndarray, nlist: int) -> faiss.Index:
        """IVF Flat –∏–Ω–¥–µ–∫—Å - CPU"""
        quantizer = faiss.IndexFlatIP(self.dimension)
        index = faiss.IndexIVFFlat(quantizer, self.dimension, nlist)
        
        logger.info(f"Training IVF index with {nlist} clusters...")
        index.train(embeddings)
        index.add(embeddings)
        
        return index
    
    def _build_ivf_pq_index(self, embeddings: np.ndarray, nlist: int, m: int = 96) -> faiss.Index:
        """IVF PQ –∏–Ω–¥–µ–∫—Å - CPU, compressed"""
        quantizer = faiss.IndexFlatIP(self.dimension)
        index = faiss.IndexIVFPQ(quantizer, self.dimension, nlist, m, 8)
        
        logger.info(f"Training IVF-PQ index with {nlist} clusters and {m} subquantizers...")
        index.train(embeddings)
        index.add(embeddings)
        
        return index
    
    def search(self, query: str, k: int = 5, nprobe: int = 10) -> List[Tuple[int, float]]:
        """–ü–æ–∏—Å–∫ —Å GPU encoding –∑–∞–ø—Ä–æ—Å–∞"""
        self._check_built()
        
        # E5 —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤
        query_text = f"query: {query}"
        
        # üöÄ Encoding –Ω–∞ GPU
        query_emb = self._encode_batch([query_text], desc=None)[0:1]
        
        # –ü–æ–∏—Å–∫ –≤ CPU –∏–Ω–¥–µ–∫—Å–µ (–±—ã—Å—Ç—Ä–æ)
        if hasattr(self.index, 'nprobe'):
            self.index.nprobe = nprobe
        
        distances, indices = self.index.search(query_emb, k)
        
        results = [
            (int(idx), float(dist))
            for idx, dist in zip(indices[0], distances[0])
            if idx >= 0
        ]
        
        return results
    
    def _encode_batch(
        self,
        texts: List[str],
        desc: Optional[str] = None
    ) -> np.ndarray:
        """–ü–∞–∫–µ—Ç–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ GPU/CPU"""
        embeddings = self.model.encode(
            texts,
            batch_size=self.batch_size,
            show_progress_bar=desc is not None,
            convert_to_numpy=True,
            normalize_embeddings=self.normalize_embeddings,
            device=self.device  # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç self.device (cuda –∏–ª–∏ cpu)
        )
        
        return embeddings.astype('float32')
    
    def save_index(self, path: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞"""
        self._check_built()
        
        path = Path(path)
        path.parent.mkdir(parents=True, exist_ok=True)
        
        faiss.write_index(self.index, str(path))
        logger.info(f"Index saved to {path}")
    
    def load_index(self, path: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞"""
        path = Path(path)
        if not path.exists():
            raise FileNotFoundError(f"Index file not found: {path}")
        
        self.index = faiss.read_index(str(path))
        self.num_documents = self.index.ntotal
        self.is_built = True
        
        logger.info(f"Index loaded from {path}: {self.num_documents} vectors")